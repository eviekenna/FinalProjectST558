---
title: "Modeling: Predicting Diabetes from Health Indicators"
author: "[Your Name]"
format:
  html:
    toc: true
    toc-depth: 3
execute:
  echo: true
  warning: false
  message: false
---

## Introduction

In this document, I build and compare predictive models for the **Diabetes_binary** outcome using the Diabetes Health Indicators dataset from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). The goal is to develop a model that can estimate the probability that an individual has prediabetes or diabetes based on health indicators such as BMI, blood pressure, cholesterol, physical activity, age, and general health status.

From the exploratory data analysis, we identified several key insights:

- **Class imbalance:** Approximately 86% of observations have no diabetes, while only 14% have prediabetes/diabetes
- **Strong predictors:** High blood pressure, high cholesterol, BMI, age, and general health status emerged as important risk factors
- **Complex relationships:** Diabetes prevalence increases with age and accumulation of multiple risk factors, suggesting non-linear patterns

I focus on two tree-based model families:

- **Classification Tree** – a single decision tree that splits the predictor space using if/else rules
- **Random Forest** – an ensemble of many decision trees fit on bootstrap samples

Throughout, I use **log-loss** (also called binary cross-entropy) as the primary performance metric because it evaluates the quality of predicted probabilities, not just hard class labels. Lower log-loss indicates better calibrated and more accurate probability predictions, which is crucial for risk assessment in medical contexts.

I first create a train/test split, define a common preprocessing recipe, and then tune each model type using 5-fold cross-validation on the training data. Finally, I compare the tuned models on the test set and select the best overall model.

## Load Packages and Data
```{r setup}
#| label: setup

# Load required packages
library(tidyverse)
library(tidymodels)  # for modeling framework
library(vip)         # for variable importance plots
library(rpart.plot)  # for tree visualization

set.seed(123)

# Set tidymodels to silence messages
tidymodels_prefer()

# Import data (already cleaned from EDA)
diabetes <- read_csv("diabetes_binary_health_indicators_BRFSS2015.csv")

# Check what the data looks like
glimpse(diabetes)
```


```{r}

diabetes <- diabetes |>
  mutate(
    Diabetes_binary = factor(Diabetes_binary, 
                            levels = c(0,1),
                            labels = c("NoDiabetes", "Diabetes")),
    
    # Convert numeric variables that should be factors
    Fruits = factor(Fruits, levels = c(0, 1), labels = c("NoFruits", "Fruits")),
    Veggies = factor(Veggies, levels = c(0, 1), labels = c("NoVeggies", "Veggies")),
    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), 
                               labels = c("NoHeavyAlc", "HeavyAlc")),
    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), 
                          labels = c("NoHealthcare", "Healthcare")),
    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), 
                        labels = c("NoDocCost", "DocCost")),
    
    # Convert GenHlth to ordered factor with labels
    GenHlth = factor(GenHlth, levels = 1:5,
                    labels = c("Excellent", "VeryGood", "Good", "Fair", "Poor"),
                    ordered = TRUE),
    
    # Ensure Age, Education, Income are ordered factors
    Age = factor(Age, ordered = TRUE),
    Education = factor(Education, ordered = TRUE),
    Income = factor(Income, ordered = TRUE)
  )

# Check the cleaned data
glimpse(diabetes)

# Check dimensions
cat("\nDataset dimensions:", nrow(diabetes), "rows ×", ncol(diabetes), "columns\n")
```

## Train/Test Split

I now split the data into a training set (70%) and a test set (30%). I stratify by the outcome to preserve the overall class imbalance in both sets.
```{r}
#| label: split

# Split into training and test sets (70/30 split)
set.seed(123)
diabetes_split <- initial_split(
  diabetes,
  prop = 0.7,
  strata = Diabetes_binary
)

diabetes_train <- training(diabetes_split)
diabetes_test  <- testing(diabetes_split)

cat("Training set size:", nrow(diabetes_train), "observations\n")
cat("Test set size:", nrow(diabetes_test), "observations\n\n")

# Check class proportions in training set
cat("Training set class distribution:\n")
diabetes_train |> 
  count(Diabetes_binary) |>
  mutate(prop = round(n / sum(n), 4))
```
```{r}
# Check class proportions in test set
cat("\nTest set class distribution:\n")
diabetes_test |> 
  count(Diabetes_binary) |>
  mutate(prop = round(n / sum(n), 4))
```

The stratified split successfully maintains the class balance (approximately 86% NoDiabetes, 14% Diabetes) in both training and test sets.

## Common Recipe for Modeling
```{r}
#| label: recipe

# Define modeling formula with key predictors identified from EDA
model_formula <- Diabetes_binary ~ HighBP + HighChol + BMI + 
                 Age + GenHlth + PhysActivity + 
                 HeartDiseaseorAttack + Stroke + DiffWalk + Sex

# Recipe that creates dummy variables for factors
diabetes_recipe <- recipe(model_formula, data = diabetes_train) |>
  # Create dummy variables for all nominal predictors
  step_dummy(all_nominal_predictors()) %>%
  # Remove predictors with zero variance
  step_zv(all_predictors())

diabetes_recipe
```

## Resampling Setup and Metric
```{r}
#| label: resampling

# Set up 5-fold stratified cross-validation 
set.seed(123)
diabetes_folds <- vfold_cv(
  diabetes_train,
  v = 5,
  strata = Diabetes_binary
)

cat("Created", nrow(diabetes_folds), "cross-validation folds\n")
cat("Each fold maintains the class distribution from the training set\n\n")

# Define the metric set with log loss
logloss_metric <- metric_set(mn_log_loss)

cat("Primary evaluation metric: Log-loss (mn_log_loss)\n")
cat("  - Measures quality of predicted probabilities\n")
cat("  - Lower values indicate better calibrated predictions\n")
cat("  - Penalizes confident incorrect predictions heavily\n")
```

## Classification Tree

### What is a Classification Tree?

A **classification tree** (also called a decision tree) is a predictive model that recursively partitions the feature space into regions using a series of if/else rules. Each terminal node (leaf) corresponds to a region of the predictor space and contains a predicted class label and probability.

**How it works:**

1. **Recursive Binary Splitting:**
   - Start with all training data at the root node
   - Find the predictor and split point that best separates the classes
   - Split the data into two child nodes based on this rule
   - Repeat recursively for each child node until a stopping criterion is met

2. **Splitting Criteria:**
   The algorithm chooses splits that maximize "purity" using measures like:
   - **Gini impurity:** Lower values indicate more pure nodes (mostly one class)
   - **Entropy:** Measures uncertainty in the node

3. **Making Predictions:**
   - Follow decision rules down the tree to a leaf node
   - Predict the majority class in that leaf
   - Probability = proportion of each class in that leaf

**Advantages:**
- Highly interpretable (easy to visualize and explain)
- No preprocessing needed (handles numeric and categorical variables)
- Automatic feature selection
- Captures non-linear relationships

**Disadvantages:**
- High variance (unstable to small data changes)
- Prone to overfitting
- Greedy algorithm (locally optimal choices)

**Key Hyperparameters:**
- **cost_complexity (cp):** Controls pruning
- **tree_depth:** Maximum depth of tree
- **min_n:** Minimum observations per node

### Tree Model Specification and Grid
```{r}
#| label: tree-spec

# Classification tree model with tunable hyperparameters
tree_spec <- decision_tree(
  cost_complexity = tune(),   # pruning parameter
  tree_depth      = tune(),   # maximum depth
  min_n           = 20        # minimum observations per node (fixed)
) %>%
  set_engine("rpart") %>%
  set_mode("classification")

tree_spec
```
```{r}
#| label: tree-workflow

# Combine tree specification with recipe
tree_workflow <- workflow() %>%
  add_model(tree_spec) %>%
  add_recipe(diabetes_recipe)

tree_workflow
```
```{r}
#| label: tree-grid

# Create tuning grid
tree_grid <- grid_regular(
  cost_complexity(range = c(-4, -1)),  # 0.0001 to 0.1
  tree_depth(range = c(3L, 10L)),      # depth 3 to 10
  levels = c(cost_complexity = 5, tree_depth = 4)  # 20 combinations
)

cat("Tuning grid contains", nrow(tree_grid), "parameter combinations\n\n")
tree_grid
```

### Tune the Tree Using 5-Fold CV
```{r}
#| label: tree-tuning
#| message: false

# Tune the classification tree
set.seed(123)
tree_res <- tune_grid(
  tree_workflow,
  resamples = diabetes_folds,
  grid = tree_grid,
  metrics = logloss_metric,
  control = control_grid(save_pred = TRUE)
)

cat("\nTree tuning complete!\n")
```

```{r}
#| label: tree-results

# Extract results
tree_metrics <- tree_res |> collect_metrics()

cat("Top 10 configurations by log-loss:\n")
tree_metrics |>
  arrange(mean) |>
  select(cost_complexity, tree_depth, mean, std_err) |>
  head(10)
```


```{r}
#| fig.width: 10
#| fig.height: 6

# Visualize tuning results
tree_metrics %>%
  ggplot(aes(x = cost_complexity, y = mean, 
             color = factor(tree_depth), group = tree_depth)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma") +
  labs(
    title = "Classification Tree: Log-loss vs Cost Complexity",
    subtitle = "5-Fold Cross-Validation Results",
    x = "Cost Complexity (log10 scale)",
    y = "Mean Log-loss",
    color = "Tree Depth"
  ) +
  theme_minimal()
```

### Select Best Tree and Evaluate
```{r}
#| label: tree-best

# Select best tree
best_tree <- tree_res %>%
  select_best(metric = "mn_log_loss")

cat("Best classification tree parameters:\n")
print(best_tree)
```
```{r}
#| label: tree-final

# Finalize and fit
final_tree_workflow <- tree_workflow %>%
  finalize_workflow(best_tree)

tree_final_fit <- last_fit(
  final_tree_workflow,
  split = diabetes_split,
  metrics = logloss_metric
)

# Test set performance
tree_test_metrics <- tree_final_fit %>% collect_metrics()

cat("\nClassification Tree - Test Set Performance:\n")
tree_test_metrics
```

## Random Forest

### What is a Random Forest?

A **random forest** is an ensemble method that combines predictions from multiple decision trees through:

1. **Bootstrap Sampling:** Each tree trained on a random sample with replacement
2. **Random Feature Selection:** At each split, only consider a random subset of features (mtry)
3. **Aggregation:** Average predictions across all trees

**Why it's better than a single tree:**
- Reduces variance through averaging
- More robust and accurate
- Less prone to overfitting
- Handles complex interactions

**Key hyperparameter: mtry** - number of features sampled at each split

### Random Forest Specification and Grid
```{r}
#| label: rf-spec

# Random forest specification
rf_spec <- rand_forest(
  mtry = tune(),   # number of predictors per split
  trees = 500,     # number of trees (fixed)
  min_n = tune()   # minimum node size
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_spec
```
```{r}
#| label: rf-workflow

# Create workflow
rf_workflow <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(diabetes_recipe)

rf_workflow
```
```{r}
#| label: rf-grid

# Calculate number of predictors
num_pred <- diabetes_recipe %>%
  prep() %>%
  bake(new_data = NULL) %>%
  select(-Diabetes_binary) %>%
  ncol()

cat("Number of predictors after encoding:", num_pred, "\n\n")

# Create tuning grid
rf_grid <- grid_regular(
  mtry(range = c(2, 8)),
  min_n(range = c(10, 40)),
  levels = 3  # This gives you 3×3 = 9 combinations instead of more
)

cat("Tuning grid contains", nrow(rf_grid), "parameter combinations\n")
```

### Tune Random Forest


```{r}
#| label: rf-tuning
#| message: false

# Tune random forest
set.seed(123)
cat("Starting random forest tuning...\n")

rf_res <- tune_grid(
  rf_workflow,
  resamples = diabetes_folds,
  grid = rf_grid,
  metrics = logloss_metric,
  control = control_grid(save_pred = TRUE)
)

cat("\nRandom forest tuning complete!\n")
```
```{r}
#| label: rf-results

# Extract results
rf_metrics <- rf_res |> collect_metrics()

cat("Top 10 configurations by log-loss:\n")
rf_metrics |>
  arrange(mean) |>
  select(mtry, min_n, mean, std_err) |>
  head(10)
```


```{r}
#| fig.width: 10
#| fig.height: 6

# Visualize results
rf_metrics %>%
  ggplot(aes(x = mtry, y = mean, color = factor(min_n), group = min_n)) +
  geom_line(size = 1.2) +
  geom_point(size = 3) +
  scale_color_viridis_d() +
  labs(
    title = "Random Forest: Log-loss vs mtry and Minimum Node Size",
    subtitle = "5-Fold Cross-Validation Results (500 trees)",
    x = "mtry (Features Sampled at Each Split)",
    y = "Mean Log-loss",
    color = "Min Node Size"
  ) +
  theme_minimal()
```

### Select Best Random Forest and Evaluate
```{r}
#| label: rf-best

# Select best configuration
best_rf <- rf_res %>%
  select_best(metric = "mn_log_loss")

cat("Best random forest parameters:\n")
print(best_rf)
```
```{r}
#| label: rf-final

# Finalize and fit
final_rf_workflow <- rf_workflow %>%
  finalize_workflow(best_rf)

rf_final_fit <- last_fit(
  final_rf_workflow,
  split = diabetes_split,
  metrics = logloss_metric
)

# Test set performance
rf_test_metrics <- rf_final_fit %>% collect_metrics()

cat("\nRandom Forest - Test Set Performance:\n")
rf_test_metrics
```

## Final Model Selection
```{r}
#| label: compare-models

# Compare both models
model_compare <- bind_rows(
  tree_test_metrics %>% mutate(model = "Classification Tree"),
  rf_test_metrics   %>% mutate(model = "Random Forest")
) %>%
  relocate(model) %>%
  arrange(.estimate)

cat(strrep("=", 60), "\n")
cat("FINAL MODEL COMPARISON - TEST SET\n")
cat(strrep("=", 60), "\n\n")

model_compare %>%
  select(Model = model, Metric = .metric, `Log-Loss` = .estimate) %>%
  mutate(`Log-Loss` = round(`Log-Loss`, 5))
```

**Winner:** The model with the lowest log-loss is selected as the final model for deployment.

## Fit Final Model on Full Dataset
```{r}
#| label: final-full-fit

# Refit winner on full dataset
final_model <- final_rf_workflow %>%
  fit(data = diabetes)

cat(strrep("=", 60), "\n")
cat("FINAL MODEL - FITTED ON FULL DATASET\n")
cat(strrep("=", 60), "\n\n")

cat("Model: Random Forest\n")
cat("  - Trees: 500\n")
cat("  - mtry: ", best_rf$mtry, "\n")
cat("  - min_n: ", best_rf$min_n, "\n")
cat("  - Training samples: ", nrow(diabetes), "\n\n")

cat("This model is ready for deployment\n")
```

## Conclusion

I successfully built and compared two tree-based models for predicting diabetes status. The random forest outperformed the single classification tree, achieving lower log-loss and demonstrating better probability calibration. The final model is trained on the full dataset and ready for deployment.

```

---

**End of Modeling Document**