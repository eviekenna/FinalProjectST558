[
  {
    "objectID": "EDA.html",
    "href": "EDA.html",
    "title": "EDA",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   4.0.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\n\nIntroduction\nThe purpose of this exploratory data analysis (EDA) is to develop a thorough understanding of the Diabetes Health Indicators (BRFSS 2015) dataset before fitting any predictive models. Understanding how the data is stored, validating variable types and ranges, checking for missingness, examining distributions, and exploring relationships between variables—this EDA identifies meaningful patterns and potential predictors of diabetes. By investigating both univariate and multivariate summaries, we aim to uncover which health behaviors, demographic factors, and clinical indicators are most strongly associated with diabetes status.\nThe ultimate goal of modeling is to build an accurate and interpretable statistical or machine learning model that can predict whether an individual has diabetes based on their health and lifestyle indicators. Insights gained from the EDA will guide variable selection, appropriate preprocessing steps, and model choice, ensuring that the final predictive model is both reliable and grounded in a clear understanding of the underlying data structure.\n\nDiabetes_df &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\nRows: 253680 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (22): Diabetes_binary, HighBP, HighChol, CholCheck, BMI, Smoker, Stroke,...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nstr(Diabetes_df)\n\nspc_tbl_ [253,680 × 22] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Diabetes_binary     : num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ HighBP              : num [1:253680] 1 0 1 1 1 1 1 1 1 0 ...\n $ HighChol            : num [1:253680] 1 0 1 0 1 1 0 1 1 0 ...\n $ CholCheck           : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ BMI                 : num [1:253680] 40 25 28 27 24 25 30 25 30 24 ...\n $ Smoker              : num [1:253680] 1 1 0 0 0 1 1 1 1 0 ...\n $ Stroke              : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ HeartDiseaseorAttack: num [1:253680] 0 0 0 0 0 0 0 0 1 0 ...\n $ PhysActivity        : num [1:253680] 0 1 0 1 1 1 0 1 0 0 ...\n $ Fruits              : num [1:253680] 0 0 1 1 1 1 0 0 1 0 ...\n $ Veggies             : num [1:253680] 1 0 0 1 1 1 0 1 1 1 ...\n $ HvyAlcoholConsump   : num [1:253680] 0 0 0 0 0 0 0 0 0 0 ...\n $ AnyHealthcare       : num [1:253680] 1 0 1 1 1 1 1 1 1 1 ...\n $ NoDocbcCost         : num [1:253680] 0 1 1 0 0 0 0 0 0 0 ...\n $ GenHlth             : num [1:253680] 5 3 5 2 2 2 3 3 5 2 ...\n $ MentHlth            : num [1:253680] 18 0 30 0 3 0 0 0 30 0 ...\n $ PhysHlth            : num [1:253680] 15 0 30 0 0 2 14 0 30 0 ...\n $ DiffWalk            : num [1:253680] 1 0 1 0 0 0 0 1 1 0 ...\n $ Sex                 : num [1:253680] 0 0 0 0 0 1 0 0 0 1 ...\n $ Age                 : num [1:253680] 9 7 9 11 11 10 9 11 9 8 ...\n $ Education           : num [1:253680] 4 6 4 3 5 6 6 4 5 4 ...\n $ Income              : num [1:253680] 3 1 8 6 4 8 7 4 1 3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   Diabetes_binary = col_double(),\n  ..   HighBP = col_double(),\n  ..   HighChol = col_double(),\n  ..   CholCheck = col_double(),\n  ..   BMI = col_double(),\n  ..   Smoker = col_double(),\n  ..   Stroke = col_double(),\n  ..   HeartDiseaseorAttack = col_double(),\n  ..   PhysActivity = col_double(),\n  ..   Fruits = col_double(),\n  ..   Veggies = col_double(),\n  ..   HvyAlcoholConsump = col_double(),\n  ..   AnyHealthcare = col_double(),\n  ..   NoDocbcCost = col_double(),\n  ..   GenHlth = col_double(),\n  ..   MentHlth = col_double(),\n  ..   PhysHlth = col_double(),\n  ..   DiffWalk = col_double(),\n  ..   Sex = col_double(),\n  ..   Age = col_double(),\n  ..   Education = col_double(),\n  ..   Income = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nAfter looking at the data there are 253,680 observations with 22 variables. The 22 variables include Binary catagrical variables (HighBP, smoker,Fruits..), Ordinal catagorical variables (age_group, education, income) and continuous varibales (BMI, phys/mental health days..).\nTo make the EDA more interpretable, columns are changed to factors to have more meaningful labels.\n\nlibrary(tidyverse)\n\nDiabetes_df &lt;- Diabetes_df |&gt;\nmutate(\nDiabetes_binary = factor(Diabetes_binary, levels = c(0, 1), labels = c(\"No Diabetes\", \"Prediabetes/Diabetes\")),\nHighBP = factor(HighBP, labels = c(\"No High BP\", \"High BP\")),\nHighChol = factor(HighChol, labels = c(\"No High Chol\", \"High Chol\")),\nCholCheck = factor(CholCheck, labels = c(\"No Check\", \"Check\")),\nSmoker = factor(Smoker, labels = c(\"Non-Smoker\", \"Smoker\")),\nStroke = factor(Stroke, labels = c(\"No Stroke\", \"Stroke\")),\nHeartDiseaseorAttack = factor(HeartDiseaseorAttack, labels = c(\"No\", \"Yes\")),\nDiffWalk = factor(DiffWalk, labels = c(\"No Difficulty\", \"Difficulty\")),\nPhysActivity = factor(PhysActivity, labels = c(\"No Activity\", \"Activity\")),\nSex = factor(Sex, labels = c(\"Female\", \"Male\")),\nAge = factor(Age),\nEducation = factor(Education),\nIncome = factor(Income)\n)\nglimpse(Diabetes_df)\n\nRows: 253,680\nColumns: 22\n$ Diabetes_binary      &lt;fct&gt; No Diabetes, No Diabetes, No Diabetes, No Diabete…\n$ HighBP               &lt;fct&gt; High BP, No High BP, High BP, High BP, High BP, H…\n$ HighChol             &lt;fct&gt; High Chol, No High Chol, High Chol, No High Chol,…\n$ CholCheck            &lt;fct&gt; Check, No Check, Check, Check, Check, Check, Chec…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;fct&gt; Smoker, Smoker, Non-Smoker, Non-Smoker, Non-Smoke…\n$ Stroke               &lt;fct&gt; No Stroke, No Stroke, No Stroke, No Stroke, No St…\n$ HeartDiseaseorAttack &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, No, No, No, …\n$ PhysActivity         &lt;fct&gt; No Activity, Activity, No Activity, Activity, Act…\n$ Fruits               &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1…\n$ Veggies              &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ HvyAlcoholConsump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AnyHealthcare        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ NoDocbcCost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ GenHlth              &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2, 3…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;fct&gt; Difficulty, No Difficulty, Difficulty, No Difficu…\n$ Sex                  &lt;fct&gt; Female, Female, Female, Female, Female, Male, Fem…\n$ Age                  &lt;fct&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;fct&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;fct&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…\n\n\n\npsych::describe(Diabetes_df)\n\n                      vars      n  mean   sd median trimmed  mad min max range\nDiabetes_binary*         1 253680  1.14 0.35      1    1.05 0.00   1   2     1\nHighBP*                  2 253680  1.43 0.49      1    1.41 0.00   1   2     1\nHighChol*                3 253680  1.42 0.49      1    1.41 0.00   1   2     1\nCholCheck*               4 253680  1.96 0.19      2    2.00 0.00   1   2     1\nBMI                      5 253680 28.38 6.61     27   27.68 4.45  12  98    86\nSmoker*                  6 253680  1.44 0.50      1    1.43 0.00   1   2     1\nStroke*                  7 253680  1.04 0.20      1    1.00 0.00   1   2     1\nHeartDiseaseorAttack*    8 253680  1.09 0.29      1    1.00 0.00   1   2     1\nPhysActivity*            9 253680  1.76 0.43      2    1.82 0.00   1   2     1\nFruits                  10 253680  0.63 0.48      1    0.67 0.00   0   1     1\nVeggies                 11 253680  0.81 0.39      1    0.89 0.00   0   1     1\nHvyAlcoholConsump       12 253680  0.06 0.23      0    0.00 0.00   0   1     1\nAnyHealthcare           13 253680  0.95 0.22      1    1.00 0.00   0   1     1\nNoDocbcCost             14 253680  0.08 0.28      0    0.00 0.00   0   1     1\nGenHlth                 15 253680  2.51 1.07      2    2.45 1.48   1   5     4\nMentHlth                16 253680  3.18 7.41      0    1.04 0.00   0  30    30\nPhysHlth                17 253680  4.24 8.72      0    1.77 0.00   0  30    30\nDiffWalk*               18 253680  1.17 0.37      1    1.09 0.00   1   2     1\nSex*                    19 253680  1.44 0.50      1    1.43 0.00   1   2     1\nAge*                    20 253680  8.03 3.05      8    8.17 2.97   1  13    12\nEducation*              21 253680  5.05 0.99      5    5.15 1.48   1   6     5\nIncome*                 22 253680  6.05 2.07      7    6.35 1.48   1   8     7\n                       skew kurtosis   se\nDiabetes_binary*       2.08     2.34 0.00\nHighBP*                0.29    -1.92 0.00\nHighChol*              0.31    -1.91 0.00\nCholCheck*            -4.88    21.83 0.00\nBMI                    2.12    11.00 0.01\nSmoker*                0.23    -1.95 0.00\nStroke*                4.66    19.69 0.00\nHeartDiseaseorAttack*  2.78     5.72 0.00\nPhysActivity*         -1.20    -0.57 0.00\nFruits                -0.56    -1.69 0.00\nVeggies               -1.59     0.54 0.00\nHvyAlcoholConsump      3.85    12.85 0.00\nAnyHealthcare         -4.18    15.48 0.00\nNoDocbcCost            3.00     6.97 0.00\nGenHlth                0.42    -0.38 0.00\nMentHlth               2.72     6.44 0.01\nPhysHlth               2.21     3.50 0.02\nDiffWalk*              1.77     1.15 0.00\nSex*                   0.24    -1.94 0.00\nAge*                  -0.36    -0.58 0.01\nEducation*            -0.78     0.04 0.00\nIncome*               -0.89    -0.28 0.00\n\n\nLooking at these the minumums and maximums of our dataset look correct as well as the medians and sd for our numeric variables.\nNow determine if there are a lot of missng values/rate of missingness for certain variables.\n\ncolSums(is.na(Diabetes_df))\n\n     Diabetes_binary               HighBP             HighChol \n                   0                    0                    0 \n           CholCheck                  BMI               Smoker \n                   0                    0                    0 \n              Stroke HeartDiseaseorAttack         PhysActivity \n                   0                    0                    0 \n              Fruits              Veggies    HvyAlcoholConsump \n                   0                    0                    0 \n       AnyHealthcare          NoDocbcCost              GenHlth \n                   0                    0                    0 \n            MentHlth             PhysHlth             DiffWalk \n                   0                    0                    0 \n                 Sex                  Age            Education \n                   0                    0                    0 \n              Income \n                   0 \n\n\nThere is no missing data so there is no need for futher invesitgation here.\nCheck the summary of the response variable\n\nDiabetes_df |&gt;\ncount(Diabetes_binary) |&gt;\nmutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Diabetes_binary           n  prop\n  &lt;fct&gt;                 &lt;int&gt; &lt;dbl&gt;\n1 No Diabetes          218334 0.861\n2 Prediabetes/Diabetes  35346 0.139\n\n\nLooking at the summary of the response variable we can see that there are far more observations of no diabetes than of prediabetes. This is important to keep in mind as we move to modeling. We can get 86% accuracy for our models simply by predicting no diabetes for the entire data set. We can tackle this by using class weights or changing our performance metric. We will follow this up later.\nLets look at a few of our distributions.\n\nlibrary(ggplot2)\n#look at distribution of BMI\nggplot(Diabetes_df, aes(BMI)) +\ngeom_histogram(bins = 40, fill = \"#2c7fb8\", alpha = 0.7) +\nlabs(title = \"Distribution of BMI\", x = \"BMI\", y = \"Count\")\n\n\n\n\n\n\n\n#look at distribution of Age\nggplot(Diabetes_df, aes(Age)) +\ngeom_bar(fill=\"#df65b0\") +\nlabs(title=\"Age Group Distribution\", x=\"Age Group\", y=\"Count\")\n\n\n\n\n\n\n\n\n\nggplot(Diabetes_df, aes(x = BMI, fill = Diabetes_binary)) +\ngeom_density(alpha = 0.4) +\nlabs(\ntitle = \"BMI Distribution by Diabetes Status\",\nx = \"BMI\", y = \"Density\",\nfill = \"Diabetes Status\"\n)\n\n\n\n\n\n\n\n\nBoth the No Diabetes and Prediabetes/Diabetes have similar distributions however, the blue curve is shifted slightly to the right so we can determine that there people with diabetes or prediabetes have a higher BMI on average.\n\n# High Blood Pressure vs Diabetes\ntable_highbp &lt;- Diabetes_df |&gt;\n  count(Diabetes_binary, HighBP) |&gt;\n  group_by(Diabetes_binary) |&gt;\n  mutate(prop = n / sum(n)) |&gt;\n  ungroup()\ntable_highbp\n\n# A tibble: 4 × 4\n  Diabetes_binary      HighBP          n  prop\n  &lt;fct&gt;                &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt;\n1 No Diabetes          No High BP 136109 0.623\n2 No Diabetes          High BP     82225 0.377\n3 Prediabetes/Diabetes No High BP   8742 0.247\n4 Prediabetes/Diabetes High BP     26604 0.753\n\n\n\nDiabetes_df |&gt;\n  count(HighChol, Diabetes_binary) |&gt;\n  pivot_wider(names_from = Diabetes_binary, values_from = n) |&gt;\n  mutate(Total = `No Diabetes` + `Prediabetes/Diabetes`,\n         `% with Diabetes` = round(`Prediabetes/Diabetes` / Total * 100, 2))\n\n# A tibble: 2 × 5\n  HighChol     `No Diabetes` `Prediabetes/Diabetes`  Total `% with Diabetes`\n  &lt;fct&gt;                &lt;int&gt;                  &lt;int&gt;  &lt;int&gt;             &lt;dbl&gt;\n1 No High Chol        134429                  11660 146089              7.98\n2 High Chol            83905                  23686 107591             22.0 \n\n\nYou can see from the above summaries as we expect high blood pressure and high cholesterol increase the percentages of prediabetes/diabetes.\nClick here for the Modeling Page"
  },
  {
    "objectID": "Modeling.html",
    "href": "Modeling.html",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "",
    "text": "In this document, I build and compare predictive models for the Diabetes_binary outcome using the Diabetes Health Indicators dataset from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). The goal is to develop a model that can estimate the probability that an individual has prediabetes or diabetes based on health indicators such as BMI, blood pressure, cholesterol, physical activity, age, and general health status.\nFrom the exploratory data analysis, we identified several key insights:\n\nClass imbalance: Approximately 86% of observations have no diabetes, while only 14% have prediabetes/diabetes\nStrong predictors: High blood pressure, high cholesterol, BMI, age, and general health status emerged as important risk factors\nComplex relationships: Diabetes prevalence increases with age and accumulation of multiple risk factors, suggesting non-linear patterns\n\nI focus on two tree-based model families:\n\nClassification Tree – a single decision tree that splits the predictor space using if/else rules\nRandom Forest – an ensemble of many decision trees fit on bootstrap samples\n\nThroughout, I use log-loss (also called binary cross-entropy) as the primary performance metric because it evaluates the quality of predicted probabilities, not just hard class labels. Lower log-loss indicates better calibrated and more accurate probability predictions, which is crucial for risk assessment in medical contexts.\nI first create a train/test split, define a common preprocessing recipe, and then tune each model type using 5-fold cross-validation on the training data. Finally, I compare the tuned models on the test set and select the best overall model."
  },
  {
    "objectID": "Modeling.html#introduction",
    "href": "Modeling.html#introduction",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "",
    "text": "In this document, I build and compare predictive models for the Diabetes_binary outcome using the Diabetes Health Indicators dataset from the 2015 Behavioral Risk Factor Surveillance System (BRFSS). The goal is to develop a model that can estimate the probability that an individual has prediabetes or diabetes based on health indicators such as BMI, blood pressure, cholesterol, physical activity, age, and general health status.\nFrom the exploratory data analysis, we identified several key insights:\n\nClass imbalance: Approximately 86% of observations have no diabetes, while only 14% have prediabetes/diabetes\nStrong predictors: High blood pressure, high cholesterol, BMI, age, and general health status emerged as important risk factors\nComplex relationships: Diabetes prevalence increases with age and accumulation of multiple risk factors, suggesting non-linear patterns\n\nI focus on two tree-based model families:\n\nClassification Tree – a single decision tree that splits the predictor space using if/else rules\nRandom Forest – an ensemble of many decision trees fit on bootstrap samples\n\nThroughout, I use log-loss (also called binary cross-entropy) as the primary performance metric because it evaluates the quality of predicted probabilities, not just hard class labels. Lower log-loss indicates better calibrated and more accurate probability predictions, which is crucial for risk assessment in medical contexts.\nI first create a train/test split, define a common preprocessing recipe, and then tune each model type using 5-fold cross-validation on the training data. Finally, I compare the tuned models on the test set and select the best overall model."
  },
  {
    "objectID": "Modeling.html#load-packages-and-data",
    "href": "Modeling.html#load-packages-and-data",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Load Packages and Data",
    "text": "Load Packages and Data\n\n# Load required packages\nlibrary(tidyverse)\nlibrary(tidymodels)  # for modeling framework\nlibrary(vip)         # for variable importance plots\nlibrary(rpart.plot)  # for tree visualization\n\nset.seed(123)\n\n# Set tidymodels to silence messages\ntidymodels_prefer()\n\n# Import data (already cleaned from EDA)\ndiabetes &lt;- read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n\n# Check what the data looks like\nglimpse(diabetes)\n\nRows: 253,680\nColumns: 22\n$ Diabetes_binary      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0…\n$ HighBP               &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n$ HighChol             &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ CholCheck            &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0…\n$ Stroke               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ HeartDiseaseorAttack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PhysActivity         &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ Fruits               &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1…\n$ Veggies              &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1…\n$ HvyAlcoholConsump    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ AnyHealthcare        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ NoDocbcCost          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ GenHlth              &lt;dbl&gt; 5, 3, 5, 2, 2, 2, 3, 3, 5, 2, 3, 3, 3, 4, 4, 2, 3…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n$ Sex                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Age                  &lt;dbl&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;dbl&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;dbl&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…\n\n\n\ndiabetes &lt;- diabetes |&gt;\n  mutate(\n    Diabetes_binary = factor(Diabetes_binary, \n                            levels = c(0,1),\n                            labels = c(\"NoDiabetes\", \"Diabetes\")),\n    \n    # Convert numeric variables that should be factors\n    Fruits = factor(Fruits, levels = c(0, 1), labels = c(\"NoFruits\", \"Fruits\")),\n    Veggies = factor(Veggies, levels = c(0, 1), labels = c(\"NoVeggies\", \"Veggies\")),\n    HvyAlcoholConsump = factor(HvyAlcoholConsump, levels = c(0, 1), \n                               labels = c(\"NoHeavyAlc\", \"HeavyAlc\")),\n    AnyHealthcare = factor(AnyHealthcare, levels = c(0, 1), \n                          labels = c(\"NoHealthcare\", \"Healthcare\")),\n    NoDocbcCost = factor(NoDocbcCost, levels = c(0, 1), \n                        labels = c(\"NoDocCost\", \"DocCost\")),\n    \n    # Convert GenHlth to ordered factor with labels\n    GenHlth = factor(GenHlth, levels = 1:5,\n                    labels = c(\"Excellent\", \"VeryGood\", \"Good\", \"Fair\", \"Poor\"),\n                    ordered = TRUE),\n    \n    # Ensure Age, Education, Income are ordered factors\n    Age = factor(Age, ordered = TRUE),\n    Education = factor(Education, ordered = TRUE),\n    Income = factor(Income, ordered = TRUE)\n  )\n\n# Check the cleaned data\nglimpse(diabetes)\n\nRows: 253,680\nColumns: 22\n$ Diabetes_binary      &lt;fct&gt; NoDiabetes, NoDiabetes, NoDiabetes, NoDiabetes, N…\n$ HighBP               &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1…\n$ HighChol             &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1…\n$ CholCheck            &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ BMI                  &lt;dbl&gt; 40, 25, 28, 27, 24, 25, 30, 25, 30, 24, 25, 34, 2…\n$ Smoker               &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0…\n$ Stroke               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n$ HeartDiseaseorAttack &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ PhysActivity         &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1…\n$ Fruits               &lt;fct&gt; NoFruits, NoFruits, Fruits, Fruits, Fruits, Fruit…\n$ Veggies              &lt;fct&gt; Veggies, NoVeggies, NoVeggies, Veggies, Veggies, …\n$ HvyAlcoholConsump    &lt;fct&gt; NoHeavyAlc, NoHeavyAlc, NoHeavyAlc, NoHeavyAlc, N…\n$ AnyHealthcare        &lt;fct&gt; Healthcare, NoHealthcare, Healthcare, Healthcare,…\n$ NoDocbcCost          &lt;fct&gt; NoDocCost, DocCost, DocCost, NoDocCost, NoDocCost…\n$ GenHlth              &lt;ord&gt; Poor, Good, Poor, VeryGood, VeryGood, VeryGood, G…\n$ MentHlth             &lt;dbl&gt; 18, 0, 30, 0, 3, 0, 0, 0, 30, 0, 0, 0, 0, 0, 30, …\n$ PhysHlth             &lt;dbl&gt; 15, 0, 30, 0, 0, 2, 14, 0, 30, 0, 0, 30, 15, 0, 2…\n$ DiffWalk             &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0…\n$ Sex                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n$ Age                  &lt;ord&gt; 9, 7, 9, 11, 11, 10, 9, 11, 9, 8, 13, 10, 7, 11, …\n$ Education            &lt;ord&gt; 4, 6, 4, 3, 5, 6, 6, 4, 5, 4, 6, 5, 5, 4, 6, 6, 4…\n$ Income               &lt;ord&gt; 3, 1, 8, 6, 4, 8, 7, 4, 1, 3, 8, 1, 7, 6, 2, 8, 3…\n\n# Check dimensions\ncat(\"\\nDataset dimensions:\", nrow(diabetes), \"rows ×\", ncol(diabetes), \"columns\\n\")\n\n\nDataset dimensions: 253680 rows × 22 columns"
  },
  {
    "objectID": "Modeling.html#traintest-split",
    "href": "Modeling.html#traintest-split",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Train/Test Split",
    "text": "Train/Test Split\nI now split the data into a training set (70%) and a test set (30%). I stratify by the outcome to preserve the overall class imbalance in both sets.\n\n# Split into training and test sets (70/30 split)\nset.seed(123)\ndiabetes_split &lt;- initial_split(\n  diabetes,\n  prop = 0.7,\n  strata = Diabetes_binary\n)\n\ndiabetes_train &lt;- training(diabetes_split)\ndiabetes_test  &lt;- testing(diabetes_split)\n\ncat(\"Training set size:\", nrow(diabetes_train), \"observations\\n\")\n\nTraining set size: 177575 observations\n\ncat(\"Test set size:\", nrow(diabetes_test), \"observations\\n\\n\")\n\nTest set size: 76105 observations\n\n# Check class proportions in training set\ncat(\"Training set class distribution:\\n\")\n\nTraining set class distribution:\n\ndiabetes_train |&gt; \n  count(Diabetes_binary) |&gt;\n  mutate(prop = round(n / sum(n), 4))\n\n# A tibble: 2 × 3\n  Diabetes_binary      n  prop\n  &lt;fct&gt;            &lt;int&gt; &lt;dbl&gt;\n1 NoDiabetes      152833 0.861\n2 Diabetes         24742 0.139\n\n\n\n# Check class proportions in test set\ncat(\"\\nTest set class distribution:\\n\")\n\n\nTest set class distribution:\n\ndiabetes_test |&gt; \n  count(Diabetes_binary) |&gt;\n  mutate(prop = round(n / sum(n), 4))\n\n# A tibble: 2 × 3\n  Diabetes_binary     n  prop\n  &lt;fct&gt;           &lt;int&gt; &lt;dbl&gt;\n1 NoDiabetes      65501 0.861\n2 Diabetes        10604 0.139\n\n\nThe stratified split successfully maintains the class balance (approximately 86% NoDiabetes, 14% Diabetes) in both training and test sets."
  },
  {
    "objectID": "Modeling.html#common-recipe-for-modeling",
    "href": "Modeling.html#common-recipe-for-modeling",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Common Recipe for Modeling",
    "text": "Common Recipe for Modeling\n\n# Define modeling formula with key predictors identified from EDA\nmodel_formula &lt;- Diabetes_binary ~ HighBP + HighChol + BMI + \n                 Age + GenHlth + PhysActivity + \n                 HeartDiseaseorAttack + Stroke + DiffWalk + Sex\n\n# Recipe that creates dummy variables for factors\ndiabetes_recipe &lt;- recipe(model_formula, data = diabetes_train) |&gt;\n  # Create dummy variables for all nominal predictors\n  step_dummy(all_nominal_predictors()) %&gt;%\n  # Remove predictors with zero variance\n  step_zv(all_predictors())\n\ndiabetes_recipe"
  },
  {
    "objectID": "Modeling.html#resampling-setup-and-metric",
    "href": "Modeling.html#resampling-setup-and-metric",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Resampling Setup and Metric",
    "text": "Resampling Setup and Metric\n\n# Set up 5-fold stratified cross-validation \nset.seed(123)\ndiabetes_folds &lt;- vfold_cv(\n  diabetes_train,\n  v = 5,\n  strata = Diabetes_binary\n)\n\ncat(\"Created\", nrow(diabetes_folds), \"cross-validation folds\\n\")\n\nCreated 5 cross-validation folds\n\ncat(\"Each fold maintains the class distribution from the training set\\n\\n\")\n\nEach fold maintains the class distribution from the training set\n\n# Define the metric set with log loss\nlogloss_metric &lt;- metric_set(mn_log_loss)\n\ncat(\"Primary evaluation metric: Log-loss (mn_log_loss)\\n\")\n\nPrimary evaluation metric: Log-loss (mn_log_loss)\n\ncat(\"  - Measures quality of predicted probabilities\\n\")\n\n  - Measures quality of predicted probabilities\n\ncat(\"  - Lower values indicate better calibrated predictions\\n\")\n\n  - Lower values indicate better calibrated predictions\n\ncat(\"  - Penalizes confident incorrect predictions heavily\\n\")\n\n  - Penalizes confident incorrect predictions heavily"
  },
  {
    "objectID": "Modeling.html#classification-tree",
    "href": "Modeling.html#classification-tree",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Classification Tree",
    "text": "Classification Tree\n\nWhat is a Classification Tree?\nA classification tree (also called a decision tree) is a predictive model that recursively partitions the feature space into regions using a series of if/else rules. Each terminal node (leaf) corresponds to a region of the predictor space and contains a predicted class label and probability.\nHow it works:\n\nRecursive Binary Splitting:\n\nStart with all training data at the root node\nFind the predictor and split point that best separates the classes\nSplit the data into two child nodes based on this rule\nRepeat recursively for each child node until a stopping criterion is met\n\nSplitting Criteria: The algorithm chooses splits that maximize “purity” using measures like:\n\nGini impurity: Lower values indicate more pure nodes (mostly one class)\nEntropy: Measures uncertainty in the node\n\nMaking Predictions:\n\nFollow decision rules down the tree to a leaf node\nPredict the majority class in that leaf\nProbability = proportion of each class in that leaf\n\n\nAdvantages: - Highly interpretable (easy to visualize and explain) - No preprocessing needed (handles numeric and categorical variables) - Automatic feature selection - Captures non-linear relationships\nDisadvantages: - High variance (unstable to small data changes) - Prone to overfitting - Greedy algorithm (locally optimal choices)\nKey Hyperparameters: - cost_complexity (cp): Controls pruning - tree_depth: Maximum depth of tree - min_n: Minimum observations per node\n\n\nTree Model Specification and Grid\n\n# Classification tree model with tunable hyperparameters\ntree_spec &lt;- decision_tree(\n  cost_complexity = tune(),   # pruning parameter\n  tree_depth      = tune(),   # maximum depth\n  min_n           = 20        # minimum observations per node (fixed)\n) %&gt;%\n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ntree_spec\n\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\n\n# Combine tree specification with recipe\ntree_workflow &lt;- workflow() %&gt;%\n  add_model(tree_spec) %&gt;%\n  add_recipe(diabetes_recipe)\n\ntree_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: decision_tree()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nDecision Tree Model Specification (classification)\n\nMain Arguments:\n  cost_complexity = tune()\n  tree_depth = tune()\n  min_n = 20\n\nComputational engine: rpart \n\n\n\n# Create tuning grid\ntree_grid &lt;- grid_regular(\n  cost_complexity(range = c(-4, -1)),  # 0.0001 to 0.1\n  tree_depth(range = c(3L, 10L)),      # depth 3 to 10\n  levels = c(cost_complexity = 5, tree_depth = 4)  # 20 combinations\n)\n\ncat(\"Tuning grid contains\", nrow(tree_grid), \"parameter combinations\\n\\n\")\n\nTuning grid contains 20 parameter combinations\n\ntree_grid\n\n# A tibble: 20 × 2\n   cost_complexity tree_depth\n             &lt;dbl&gt;      &lt;int&gt;\n 1        0.0001            3\n 2        0.000562          3\n 3        0.00316           3\n 4        0.0178            3\n 5        0.1               3\n 6        0.0001            5\n 7        0.000562          5\n 8        0.00316           5\n 9        0.0178            5\n10        0.1               5\n11        0.0001            7\n12        0.000562          7\n13        0.00316           7\n14        0.0178            7\n15        0.1               7\n16        0.0001           10\n17        0.000562         10\n18        0.00316          10\n19        0.0178           10\n20        0.1              10\n\n\n\n\nTune the Tree Using 5-Fold CV\n\n# Tune the classification tree\nset.seed(123)\ntree_res &lt;- tune_grid(\n  tree_workflow,\n  resamples = diabetes_folds,\n  grid = tree_grid,\n  metrics = logloss_metric,\n  control = control_grid(save_pred = TRUE)\n)\n\ncat(\"\\nTree tuning complete!\\n\")\n\n\nTree tuning complete!\n\n\n\n# Extract results\ntree_metrics &lt;- tree_res |&gt; collect_metrics()\n\ncat(\"Top 10 configurations by log-loss:\\n\")\n\nTop 10 configurations by log-loss:\n\ntree_metrics |&gt;\n  arrange(mean) |&gt;\n  select(cost_complexity, tree_depth, mean, std_err) |&gt;\n  head(10)\n\n# A tibble: 10 × 4\n   cost_complexity tree_depth  mean    std_err\n             &lt;dbl&gt;      &lt;int&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1        0.0001           10 0.338 0.00348   \n 2        0.0001            7 0.347 0.00348   \n 3        0.000562         10 0.352 0.000710  \n 4        0.000562          7 0.352 0.000718  \n 5        0.0001            5 0.353 0.000728  \n 6        0.000562          5 0.353 0.000728  \n 7        0.00316           5 0.353 0.000728  \n 8        0.00316           7 0.353 0.000728  \n 9        0.00316          10 0.353 0.000728  \n10        0.0001            3 0.404 0.00000973\n\n\n\n# Visualize tuning results\ntree_metrics %&gt;%\n  ggplot(aes(x = cost_complexity, y = mean, \n             color = factor(tree_depth), group = tree_depth)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_x_log10(labels = scales::label_number()) +\n  scale_color_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Classification Tree: Log-loss vs Cost Complexity\",\n    subtitle = \"5-Fold Cross-Validation Results\",\n    x = \"Cost Complexity (log10 scale)\",\n    y = \"Mean Log-loss\",\n    color = \"Tree Depth\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSelect Best Tree and Evaluate\n\n# Select best tree\nbest_tree &lt;- tree_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\ncat(\"Best classification tree parameters:\\n\")\n\nBest classification tree parameters:\n\nprint(best_tree)\n\n# A tibble: 1 × 3\n  cost_complexity tree_depth .config         \n            &lt;dbl&gt;      &lt;int&gt; &lt;chr&gt;           \n1          0.0001         10 pre0_mod04_post0\n\n\n\n# Finalize and fit\nfinal_tree_workflow &lt;- tree_workflow %&gt;%\n  finalize_workflow(best_tree)\n\ntree_final_fit &lt;- last_fit(\n  final_tree_workflow,\n  split = diabetes_split,\n  metrics = logloss_metric\n)\n\n# Test set performance\ntree_test_metrics &lt;- tree_final_fit %&gt;% collect_metrics()\n\ncat(\"\\nClassification Tree - Test Set Performance:\\n\")\n\n\nClassification Tree - Test Set Performance:\n\ntree_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.346 pre0_mod0_post0"
  },
  {
    "objectID": "Modeling.html#random-forest",
    "href": "Modeling.html#random-forest",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Random Forest",
    "text": "Random Forest\n\nWhat is a Random Forest?\nA random forest is an ensemble method that combines predictions from multiple decision trees through:\n\nBootstrap Sampling: Each tree trained on a random sample with replacement\nRandom Feature Selection: At each split, only consider a random subset of features (mtry)\nAggregation: Average predictions across all trees\n\nWhy it’s better than a single tree: - Reduces variance through averaging - More robust and accurate - Less prone to overfitting - Handles complex interactions\nKey hyperparameter: mtry - number of features sampled at each split\n\n\nRandom Forest Specification and Grid\n\n# Random forest specification\nrf_spec &lt;- rand_forest(\n  mtry = tune(),   # number of predictors per split\n  trees = 500,     # number of trees (fixed)\n  min_n = tune()   # minimum node size\n) %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"classification\")\n\nrf_spec\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\n\n# Create workflow\nrf_workflow &lt;- workflow() %&gt;%\n  add_model(rf_spec) %&gt;%\n  add_recipe(diabetes_recipe)\n\nrf_workflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: rand_forest()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = tune()\n  trees = 500\n  min_n = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\n\n# Calculate number of predictors\nnum_pred &lt;- diabetes_recipe %&gt;%\n  prep() %&gt;%\n  bake(new_data = NULL) %&gt;%\n  select(-Diabetes_binary) %&gt;%\n  ncol()\n\ncat(\"Number of predictors after encoding:\", num_pred, \"\\n\\n\")\n\nNumber of predictors after encoding: 24 \n\n# Create tuning grid\nrf_grid &lt;- grid_regular(\n  mtry(range = c(2, 8)),\n  min_n(range = c(10, 40)),\n  levels = 3  # This gives you 3×3 = 9 combinations instead of more\n)\n\ncat(\"Tuning grid contains\", nrow(rf_grid), \"parameter combinations\\n\")\n\nTuning grid contains 9 parameter combinations\n\n\n\n\nTune Random Forest\n\n# Tune random forest\nset.seed(123)\ncat(\"Starting random forest tuning...\\n\")\n\nStarting random forest tuning...\n\nrf_res &lt;- tune_grid(\n  rf_workflow,\n  resamples = diabetes_folds,\n  grid = rf_grid,\n  metrics = logloss_metric,\n  control = control_grid(save_pred = TRUE)\n)\n\ncat(\"\\nRandom forest tuning complete!\\n\")\n\n\nRandom forest tuning complete!\n\n\n\n# Extract results\nrf_metrics &lt;- rf_res |&gt; collect_metrics()\n\ncat(\"Top 10 configurations by log-loss:\\n\")\n\nTop 10 configurations by log-loss:\n\nrf_metrics |&gt;\n  arrange(mean) |&gt;\n  select(mtry, min_n, mean, std_err) |&gt;\n  head(10)\n\n# A tibble: 9 × 4\n   mtry min_n  mean  std_err\n  &lt;int&gt; &lt;int&gt; &lt;dbl&gt;    &lt;dbl&gt;\n1     5    40 0.318 0.000707\n2     5    25 0.318 0.000695\n3     5    10 0.319 0.000746\n4     8    40 0.321 0.000730\n5     2    40 0.321 0.000508\n6     2    10 0.321 0.000525\n7     2    25 0.322 0.000587\n8     8    25 0.322 0.000736\n9     8    10 0.326 0.000815\n\n\n\n# Visualize results\nrf_metrics %&gt;%\n  ggplot(aes(x = mtry, y = mean, color = factor(min_n), group = min_n)) +\n  geom_line(size = 1.2) +\n  geom_point(size = 3) +\n  scale_color_viridis_d() +\n  labs(\n    title = \"Random Forest: Log-loss vs mtry and Minimum Node Size\",\n    subtitle = \"5-Fold Cross-Validation Results (500 trees)\",\n    x = \"mtry (Features Sampled at Each Split)\",\n    y = \"Mean Log-loss\",\n    color = \"Min Node Size\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nSelect Best Random Forest and Evaluate\n\n# Select best configuration\nbest_rf &lt;- rf_res %&gt;%\n  select_best(metric = \"mn_log_loss\")\n\ncat(\"Best random forest parameters:\\n\")\n\nBest random forest parameters:\n\nprint(best_rf)\n\n# A tibble: 1 × 3\n   mtry min_n .config        \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;          \n1     5    40 pre0_mod6_post0\n\n\n\n# Finalize and fit\nfinal_rf_workflow &lt;- rf_workflow %&gt;%\n  finalize_workflow(best_rf)\n\nrf_final_fit &lt;- last_fit(\n  final_rf_workflow,\n  split = diabetes_split,\n  metrics = logloss_metric\n)\n\n# Test set performance\nrf_test_metrics &lt;- rf_final_fit %&gt;% collect_metrics()\n\ncat(\"\\nRandom Forest - Test Set Performance:\\n\")\n\n\nRandom Forest - Test Set Performance:\n\nrf_test_metrics\n\n# A tibble: 1 × 4\n  .metric     .estimator .estimate .config        \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;          \n1 mn_log_loss binary         0.318 pre0_mod0_post0"
  },
  {
    "objectID": "Modeling.html#final-model-selection",
    "href": "Modeling.html#final-model-selection",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Final Model Selection",
    "text": "Final Model Selection\n\n# Compare both models\nmodel_compare &lt;- bind_rows(\n  tree_test_metrics %&gt;% mutate(model = \"Classification Tree\"),\n  rf_test_metrics   %&gt;% mutate(model = \"Random Forest\")\n) %&gt;%\n  relocate(model) %&gt;%\n  arrange(.estimate)\n\ncat(strrep(\"=\", 60), \"\\n\")\n\n============================================================ \n\ncat(\"FINAL MODEL COMPARISON - TEST SET\\n\")\n\nFINAL MODEL COMPARISON - TEST SET\n\ncat(strrep(\"=\", 60), \"\\n\\n\")\n\n============================================================ \n\nmodel_compare %&gt;%\n  select(Model = model, Metric = .metric, `Log-Loss` = .estimate) %&gt;%\n  mutate(`Log-Loss` = round(`Log-Loss`, 5))\n\n# A tibble: 2 × 3\n  Model               Metric      `Log-Loss`\n  &lt;chr&gt;               &lt;chr&gt;            &lt;dbl&gt;\n1 Random Forest       mn_log_loss      0.318\n2 Classification Tree mn_log_loss      0.346\n\n\nWinner: The model with the lowest log-loss is selected as the final model for deployment."
  },
  {
    "objectID": "Modeling.html#fit-final-model-on-full-dataset",
    "href": "Modeling.html#fit-final-model-on-full-dataset",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Fit Final Model on Full Dataset",
    "text": "Fit Final Model on Full Dataset\n\n# Refit winner on full dataset\nfinal_model &lt;- final_rf_workflow %&gt;%\n  fit(data = diabetes)\n\ncat(strrep(\"=\", 60), \"\\n\")\n\n============================================================ \n\ncat(\"FINAL MODEL - FITTED ON FULL DATASET\\n\")\n\nFINAL MODEL - FITTED ON FULL DATASET\n\ncat(strrep(\"=\", 60), \"\\n\\n\")\n\n============================================================ \n\ncat(\"Model: Random Forest\\n\")\n\nModel: Random Forest\n\ncat(\"  - Trees: 500\\n\")\n\n  - Trees: 500\n\ncat(\"  - mtry: \", best_rf$mtry, \"\\n\")\n\n  - mtry:  5 \n\ncat(\"  - min_n: \", best_rf$min_n, \"\\n\")\n\n  - min_n:  40 \n\ncat(\"  - Training samples: \", nrow(diabetes), \"\\n\\n\")\n\n  - Training samples:  253680 \n\ncat(\"This model is ready for deployment\\n\")\n\nThis model is ready for deployment"
  },
  {
    "objectID": "Modeling.html#conclusion",
    "href": "Modeling.html#conclusion",
    "title": "Modeling: Predicting Diabetes from Health Indicators",
    "section": "Conclusion",
    "text": "Conclusion\nI successfully built and compared two tree-based models for predicting diabetes status. The random forest outperformed the single classification tree, achieving lower log-loss and demonstrating better probability calibration. The final model is trained on the full dataset and ready for deployment.\n```\n\nEnd of Modeling Document"
  }
]